{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
   }
  },
  "interpreter": {
   "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_1 pdf loaded\n"
     ]
    }
   ],
   "source": [
    "# pip install pdfx\n",
    "import pdfx\n",
    "# pdf = pdfx.PDFx(\"http://europepmc.org/articles/PMC3001474?pdf=render\")\n",
    "pdf = pdfx.PDFx(\"/Users/brunoflaven/Documents/01_work/blog_articles/extending_streamlit_usage/003_99_ambitieuses_derwenai_spacy_tutorials/article_bf_2.pdf\")\n",
    "print(\"\\n--- step_1 pdf loaded\")\n",
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_2 text loaded\n"
     ]
    }
   ],
   "source": [
    "text = pdf.get_text()\n",
    "# text\n",
    "print(\"\\n--- step_2 text loaded\")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_3 spacy loaded\n\n--- step_3a require cleaning spacy doc object\n{'our', 'either', 'whereafter', 'whom', 'any', 'two', 'rather', 'own', 'nine', 'whereby', 'why', 'but', 'used', 'hers', 'him', 'themselves', 'be', 'someone', '‘m', \"'d\", 'became', 'might', 'myself', 'nevertheless', 'per', 'above', 'last', 'seems', 'therein', 'himself', 'all', 'while', 'due', 'show', '‘d', 'did', 'forty', 'formerly', 'had', 'you', 'who', \"n't\", 'those', 'perhaps', 'well', 'more', 'whereas', 'within', 'nor', 'whose', 'call', 'he', 'becoming', 'please', 'thereby', 'go', 'enough', '’ll', '’re', '‘ll', 'elsewhere', 'throughout', 'during', 'anyone', 'of', 'n‘t', 'indeed', 'latter', 'thereafter', 'top', 'least', 'whether', 'several', 'using', 'not', 'mine', 'take', 'am', 'she', 'everywhere', 'six', 'thus', 'somehow', 'have', 'fifty', 'every', 'such', 'various', 'herself', 'otherwise', 'many', 'i', 'almost', 'there', 'five', 'cannot', 'noone', 'ca', 'an', 'herein', 'hereafter', 'again', 'name', 'anyhow', 'eight', 'was', 'as', 'will', 'nobody', 'should', 'whole', 'anything', 'move', 'off', 'keep', 'his', 'everything', 'four', 'or', 'twelve', 'under', 'regarding', 'towards', 'everyone', 'ourselves', 'side', 'then', \"'ve\", 'also', 'anyway', 'moreover', 'down', 'until', 'still', 'see', 'here', 'however', 'than', 'really', '’d', 'wherever', 'could', 'the', 'yours', 'hence', 'same', 'something', 'sixty', 'say', 'wherein', 'their', 'your', 'seemed', 'get', \"'re\", 'former', 'n’t', 'my', 'neither', 'behind', 'always', 'both', 'before', 'through', 'a', 'empty', 'become', 'yourself', '‘ve', 'eleven', 'among', 'other', 'would', 'no', 'most', 'upon', \"'ll\", 'bottom', 'were', 'therefore', 'up', 'over', 'anywhere', '‘re', 'in', 'hundred', 'front', 'sometime', 'others', 'us', 'back', 'only', 'three', 'we', 'besides', 'put', 'hereby', 'around', 'whatever', 'and', 'some', 'onto', 'ever', 'full', 'seeming', 'amongst', 'whereupon', 'part', '’m', 'because', 'ten', '’ve', 'without', 'into', 'how', 'may', 'quite', 'few', 'beyond', 'where', 'me', 'at', 'done', 'against', 'whence', 'they', 'too', 'once', 'seem', 'after', 'for', 'along', 'together', 'becomes', 'thru', 'nothing', 'just', 'somewhere', 'from', 'between', 'doing', 'one', 'it', 'meanwhile', 'what', 'beside', 'are', 'on', 'toward', 'to', 're', 'her', 'made', '’s', 'them', 'even', 'its', 'whoever', 'via', 'unless', 'hereupon', 'does', 'so', 'else', 'do', 'thereupon', 'below', 'across', 'amount', 'is', 'sometimes', 'afterwards', 'if', 'these', 'this', 'ours', \"'s\", 'yourselves', 'whenever', 'nowhere', 'third', 'has', 'itself', 'yet', \"'m\", 'less', 'by', 'fifteen', 'now', 'been', 'being', 'much', 'latterly', 'further', 'except', 'which', 'already', 'though', 'serious', 'that', 'first', 'very', 'next', 'mostly', 'when', 'alone', 'can', 'since', 'about', 'thence', 'namely', 'give', 'beforehand', 'out', 'each', 'twenty', 'none', 'whither', 'make', 'often', 'must', 'although', 'with', 'another', '‘s', 'never'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load('fr_core_news_sm')\n",
    "print(\"\\n--- step_3 spacy loaded\")\n",
    "\n",
    "print(\"\\n--- step_3a require cleaning spacy doc object\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)\n",
    "len(STOP_WORDS)\n",
    "\n",
    "nlp_ambitieuses_text=nlp(text)\n",
    "# print(nlp_ambitieuses_text)\n",
    "\n",
    "# print(\"Non Stop words Below that: \")\n",
    "for word in nlp_ambitieuses_text:\n",
    "    if word.is_stop==False:\n",
    "        # print(word)\n",
    "        word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_4 spacy doc object into pandas dataframe\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              text          lemma    POS      explain  stopword\n",
       "0           Python         Python  PROPN  proper noun     False\n",
       "1                ,              ,  PUNCT  punctuation     False\n",
       "2    Randomization  Randomization  PROPN  proper noun     False\n",
       "3                ,              ,  PUNCT  punctuation     False\n",
       "4              E2E            E2E  PROPN  proper noun     False\n",
       "..             ...            ...    ...          ...       ...\n",
       "492              ,              ,  PUNCT  punctuation     False\n",
       "493            let            let   VERB         verb     False\n",
       "494           code           code   NOUN         noun     False\n",
       "495            ...            ...  PUNCT  punctuation     False\n",
       "496    \\n \\n \\n\\n\n",
       "    \\n \\n \\n\\n\n",
       "  SPACE        space     False\n",
       "\n",
       "[497 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>lemma</th>\n      <th>POS</th>\n      <th>explain</th>\n      <th>stopword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Python</td>\n      <td>Python</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td>punctuation</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Randomization</td>\n      <td>Randomization</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td>punctuation</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>E2E</td>\n      <td>E2E</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>492</th>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td>punctuation</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>493</th>\n      <td>let</td>\n      <td>let</td>\n      <td>VERB</td>\n      <td>verb</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>494</th>\n      <td>code</td>\n      <td>code</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>...</td>\n      <td>...</td>\n      <td>PUNCT</td>\n      <td>punctuation</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>\\n \\n \\n\\n</td>\n      <td>\\n \\n \\n\\n</td>\n      <td>SPACE</td>\n      <td>space</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>497 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_4 spacy doc object into pandas dataframe\")\n",
    "# GREAT load the spacy doc object into into a dataframe of the parsed tokens\n",
    "import pandas as pd\n",
    "\n",
    "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "rows = []\n",
    "\n",
    "# for t in doc:\n",
    "for t in nlp_ambitieuses_text:\n",
    "    if t.is_stop==False:\n",
    "        # print(word)\n",
    "        # word\n",
    "        row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "pd.set_option('max_colwidth',150)\n",
    "data_df = pd.DataFrame(rows, columns=cols)\n",
    "data_df = data_df.sort_index()\n",
    "\n",
    "# OUTPUT\n",
    "data_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}