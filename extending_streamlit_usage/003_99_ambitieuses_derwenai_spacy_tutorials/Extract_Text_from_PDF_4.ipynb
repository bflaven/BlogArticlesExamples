{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
   }
  },
  "interpreter": {
   "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_1 pdf loaded\n"
     ]
    }
   ],
   "source": [
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "\n",
    "# pip install pdfx\n",
    "import pdfx\n",
    "# pdf = pdfx.PDFx(\"http://europepmc.org/articles/PMC3001474?pdf=render\")\n",
    "pdf = pdfx.PDFx(\"/Users/brunoflaven/Documents/01_work/blog_articles/extending_streamlit_usage/003_99_ambitieuses_derwenai_spacy_tutorials/article_bf_2.pdf\")\n",
    "print(\"\\n--- step_1 pdf loaded\")\n",
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_2 text loaded\n"
     ]
    }
   ],
   "source": [
    "text = pdf.get_text()\n",
    "# text\n",
    "print(\"\\n--- step_2 text loaded\")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_3 spacy loaded\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(text)\n",
    "print(\"\\n--- step_3 spacy loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_4 spacy in treatment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_4 spacy in treatment\")\n",
    "# 1. Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token.text for token in doc]\n",
    "# print(tokens)\n",
    "\n",
    "# 2. Creating and updating our list of filtered tokens using list comprehension \n",
    "# filtered = [token.text for token in doc if token.is_stop == False]\n",
    "# print(filtered)\n",
    "\n",
    "# 3. Remove punctuation from our text as well using \"isalpha\" method of string objects and using list comprehensions.\n",
    "filtered = [token.text for token in doc if token.is_stop == False and token.text.isalpha() == True]\n",
    "# print(filtered)\n",
    "\n",
    "# Source : https://iq.opengenus.org/text-preprocessing-in-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_5 spacy pos\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- step_5 spacy pos\")\n",
    "#Extracting POS\n",
    "# pos = [[token.text,token.pos_] for token in doc]\n",
    "# print (pos)\n",
    "\n",
    "pos = [[token.text,token.pos_] for token in doc if token.is_stop == False and token.text.isalpha() == True]\n",
    "# print (pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_6 spacy extracting entities \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_6 spacy extracting entities \")\n",
    "# extracting entities \n",
    "entities=[(i, i.label_, i.label) for i in doc.ents]\n",
    "# print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_7 spacy doc object into pandas dataframe\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              text          lemma    POS      explain  stopword\n",
       "0           Python         Python  PROPN  proper noun     False\n",
       "1    Randomization  Randomization  PROPN  proper noun     False\n",
       "2           Random         Random  PROPN  proper noun     False\n",
       "3             good           good    ADJ    adjective     False\n",
       "4          reasons         reason   NOUN         noun     False\n",
       "..             ...            ...    ...          ...       ...\n",
       "295           mayo           mayo   NOUN         noun     False\n",
       "296       sandwich       sandwich   NOUN         noun     False\n",
       "297        concept        concept   NOUN         noun     False\n",
       "298            let            let   VERB         verb     False\n",
       "299           code           code   NOUN         noun     False\n",
       "\n",
       "[300 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>lemma</th>\n      <th>POS</th>\n      <th>explain</th>\n      <th>stopword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Python</td>\n      <td>Python</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Randomization</td>\n      <td>Randomization</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Random</td>\n      <td>Random</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good</td>\n      <td>good</td>\n      <td>ADJ</td>\n      <td>adjective</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>reasons</td>\n      <td>reason</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>mayo</td>\n      <td>mayo</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>sandwich</td>\n      <td>sandwich</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>concept</td>\n      <td>concept</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>let</td>\n      <td>let</td>\n      <td>VERB</td>\n      <td>verb</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>code</td>\n      <td>code</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_7 spacy doc object into pandas dataframe\")\n",
    "# GREAT load the spacy doc object into into a dataframe of the parsed tokens\n",
    "import pandas as pd\n",
    "\n",
    "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "rows = []\n",
    "\n",
    "for t in doc:\n",
    "    if t.is_stop == False and t.text.isalpha() == True:\n",
    "        # print(t.text)\n",
    "        row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "        rows.append(row)\n",
    "\n",
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "pd.set_option('max_colwidth',150)\n",
    "data_df = pd.DataFrame(rows, columns=cols)\n",
    "data_df = data_df.sort_index()\n",
    "\n",
    "# OUTPUT\n",
    "# data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 300 entries, 0 to 299\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   text      300 non-null    object\n 1   lemma     300 non-null    object\n 2   POS       300 non-null    object\n 3   explain   300 non-null    object\n 4   stopword  300 non-null    bool  \ndtypes: bool(1), object(4)\nmemory usage: 9.8+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data_df.sample(5)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}