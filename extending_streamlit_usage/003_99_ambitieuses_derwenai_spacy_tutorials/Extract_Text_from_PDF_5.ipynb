{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
   }
  },
  "interpreter": {
   "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_1 pdf loaded\n"
     ]
    }
   ],
   "source": [
    "## for data\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "## for text processing\n",
    "import re\n",
    "import nltk\n",
    "## for language detection\n",
    "import langdetect \n",
    "## for sentiment\n",
    "from textblob import TextBlob\n",
    "## for ner\n",
    "import spacy\n",
    "## for vectorizer\n",
    "from sklearn import feature_extraction, manifold\n",
    "## for word embedding\n",
    "import gensim.downloader as gensim_api\n",
    "## for topic modeling\n",
    "import gensim\n",
    "\n",
    "# pip install pdfx\n",
    "import pdfx\n",
    "# pdf = pdfx.PDFx(\"http://europepmc.org/articles/PMC3001474?pdf=render\")\n",
    "pdf = pdfx.PDFx(\"/Users/brunoflaven/Documents/01_work/blog_articles/extending_streamlit_usage/003_99_ambitieuses_derwenai_spacy_tutorials/article_bf_2.pdf\")\n",
    "print(\"\\n--- step_1 pdf loaded\")\n",
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_2 text loaded\n"
     ]
    }
   ],
   "source": [
    "text = pdf.get_text()\n",
    "# text\n",
    "print(\"\\n--- step_2 text loaded\")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_3 spacy loaded\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(text)\n",
    "print(\"\\n--- step_3 spacy loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_4 spacy in treatment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_4 spacy in treatment\")\n",
    "# 1. Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token.text for token in doc]\n",
    "# print(tokens)\n",
    "\n",
    "# 2. Creating and updating our list of filtered tokens using list comprehension \n",
    "# filtered = [token.text for token in doc if token.is_stop == False]\n",
    "# print(filtered)\n",
    "\n",
    "# 3. Remove punctuation from our text as well using \"isalpha\" method of string objects and using list comprehensions.\n",
    "filtered = [token.text for token in doc if token.is_stop == False and token.text.isalpha() == True]\n",
    "# print(filtered)\n",
    "\n",
    "# Source : https://iq.opengenus.org/text-preprocessing-in-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_5 spacy pos\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- step_5 spacy pos\")\n",
    "#Extracting POS\n",
    "# pos = [[token.text,token.pos_] for token in doc]\n",
    "# print (pos)\n",
    "\n",
    "pos = [[token.text,token.pos_] for token in doc if token.is_stop == False and token.text.isalpha() == True]\n",
    "# print (pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_6 spacy extracting entities \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_6 spacy extracting entities \")\n",
    "# extracting entities \n",
    "entities=[(i, i.label_, i.label) for i in doc.ents]\n",
    "# print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_7 spacy doc object into pandas dataframe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_7 spacy doc object into pandas dataframe\")\n",
    "# GREAT load the spacy doc object into into a dataframe of the parsed tokens\n",
    "import pandas as pd\n",
    "\n",
    "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "rows = []\n",
    "\n",
    "for t in doc:\n",
    "    if t.is_stop == False and t.text.isalpha() == True:\n",
    "        # print(t.text)\n",
    "        row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "        rows.append(row)\n",
    "\n",
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "pd.set_option('max_colwidth',150)\n",
    "data_df = pd.DataFrame(rows, columns=cols)\n",
    "data_df = data_df.sort_index()\n",
    "\n",
    "# OUTPUT\n",
    "# data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         text     lemma    POS      explain  stopword\n",
       "88       bias      bias   NOUN         noun     False\n",
       "56        bad       bad    ADJ    adjective     False\n",
       "97    testing   testing   NOUN         noun     False\n",
       "177    Python    Python  PROPN  proper noun     False\n",
       "263  complete  complete    ADJ    adjective     False"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>lemma</th>\n      <th>POS</th>\n      <th>explain</th>\n      <th>stopword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>88</th>\n      <td>bias</td>\n      <td>bias</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>bad</td>\n      <td>bad</td>\n      <td>ADJ</td>\n      <td>adjective</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>testing</td>\n      <td>testing</td>\n      <td>NOUN</td>\n      <td>noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>Python</td>\n      <td>Python</td>\n      <td>PROPN</td>\n      <td>proper noun</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>263</th>\n      <td>complete</td>\n      <td>complete</td>\n      <td>ADJ</td>\n      <td>adjective</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\n",
    "data_df.sample(5)\n",
    "# data_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}