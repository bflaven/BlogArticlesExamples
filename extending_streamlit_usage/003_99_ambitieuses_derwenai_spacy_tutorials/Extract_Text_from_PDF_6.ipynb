{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
   }
  },
  "interpreter": {
   "hash": "15ce07131e6ac9ef2286e9dc443e97a8b7a1385e6fe0cfbd91ad33021bc29612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_1 pdf loaded\n"
     ]
    }
   ],
   "source": [
    "## for data\n",
    "import pandas as pd\n",
    "import collections\n",
    "import json\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "## for text processing\n",
    "import re\n",
    "import nltk\n",
    "## for language detection\n",
    "import langdetect \n",
    "## for sentiment\n",
    "from textblob import TextBlob\n",
    "## for ner\n",
    "import spacy\n",
    "## for vectorizer\n",
    "from sklearn import feature_extraction, manifold\n",
    "## for word embedding\n",
    "import gensim.downloader as gensim_api\n",
    "## for topic modeling\n",
    "import gensim\n",
    "\n",
    "# pip install pdfx\n",
    "import pdfx\n",
    "# pdf = pdfx.PDFx(\"http://europepmc.org/articles/PMC3001474?pdf=render\")\n",
    "pdf = pdfx.PDFx(\"/Users/brunoflaven/Documents/01_work/blog_articles/extending_streamlit_usage/003_99_ambitieuses_derwenai_spacy_tutorials/article_bf_2.pdf\")\n",
    "print(\"\\n--- step_1 pdf loaded\")\n",
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_2 text loaded\n"
     ]
    }
   ],
   "source": [
    "text = pdf.get_text()\n",
    "# text\n",
    "print(\"\\n--- step_2 text loaded\")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_3 spacy loaded\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# doc is used below\n",
    "doc = nlp(text)\n",
    "print(\"\\n--- step_3 spacy loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_4 spacy in treatment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_4 spacy in treatment\")\n",
    "# 1. Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token.text for token in doc]\n",
    "# print(tokens)\n",
    "\n",
    "# 2. Creating and updating our list of filtered tokens using list comprehension \n",
    "# filtered = [token.text for token in doc if token.is_stop == False]\n",
    "# print(filtered)\n",
    "\n",
    "# 3. Remove punctuation from our text as well using \"isalpha\" method of string objects and using list comprehensions.\n",
    "filtered = [token.text for token in doc if token.is_stop == False and token.text.isalpha() == True]\n",
    "# print(filtered)\n",
    "\n",
    "# Source : https://iq.opengenus.org/text-preprocessing-in-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_5 spacy pos\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- step_5 spacy pos\")\n",
    "#Extracting POS\n",
    "# pos = [[token.text,token.pos_] for token in doc]\n",
    "# print (pos)\n",
    "\n",
    "pos = [[token.text,token.pos_] for token in doc if token.is_stop == False and token.text.isalpha() == True]\n",
    "# print (pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- step_6 spacy extracting entities \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- step_6 spacy extracting entities \")\n",
    "# extracting entities \n",
    "entities=[(i, i.label_, i.label) for i in doc.ents]\n",
    "# print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Python', 11), ('order', 4), ('Order', 3), ('testing', 3), ('execution', 3), ('bias', 3), ('reasons', 2), ('relative', 2), ('world', 2), ('time', 2), ('experience', 2), ('discovered', 2), ('disorder', 2), ('efficient', 2), ('easy', 2), ('called', 2), ('post', 2), ('sense', 2), ('bad', 2), ('introduce', 2)]\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(text)\n",
    "# complete_doc = nlp(complete_text)\n",
    "complete_doc = nlp(text)\n",
    "\n",
    "# Remove stop words and punctuation symbols\n",
    "# words = [token.text for token in complete_doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "words = [token.text for token in complete_doc if token.is_stop == False and not token.is_punct and token.text.isalpha() == True]\n",
    "\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(20)\n",
    "print (common_words)\n",
    "\n",
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "# print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}